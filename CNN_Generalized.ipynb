{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.datasets\n",
    "from keras.datasets import fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "import pandas as pd\n",
    "import hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag','Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params = {\n",
    "    #Training Level Hyper Params\n",
    "    \"epochs\": 200,\n",
    "    \"lr\": 0.001,\n",
    "    \"batch_size\": 2048,\n",
    "    \"opt\": keras.optimizers.Adam,\n",
    "    #Network Level HyperParam\n",
    "    \"conv_layers\":{\n",
    "        1: [Conv2D(32, kernel_size=(3, 3), activation='linear', input_shape=(28,28,1),padding='same',strides=(2,2) )],\n",
    "        2: [LeakyReLU(alpha=0.1)],\n",
    "        3: [keras.layers.BatchNormalization()],\n",
    "        4: [MaxPooling2D((2, 2),padding='same')],\n",
    "        \n",
    "    },\n",
    "    \"conn_layers\":{\n",
    "        1: [Dense(128, activation='linear')],\n",
    "        2: [LeakyReLU(alpha=0.1)],\n",
    "        3: [Dense(10, activation='softmax')],\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionData:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Setup train and test splits\n",
    "        (self.x_train, self.y_train), (self.x_test, self.y_test) = fashion_mnist.load_data()\n",
    "        print(\"Training label shape: \", self.y_train.shape) \n",
    "        print(\"Training Data Shape\", self.x_train.shape)\n",
    "    \n",
    "    def MakeGenericModel (self, hyperP):\n",
    "        \n",
    "            modelH = Sequential()\n",
    "            \n",
    "            for i in range(1, len(hyperP['conv_layers'])+1):\n",
    "                modelH.add(hyperP['conv_layers'][i][0])\n",
    "            modelH.add(Flatten())\n",
    "            for i in range(1, len(hyperP['conn_layers'])+1):\n",
    "                modelH.add(hyperP['conn_layers'][i][0])\n",
    "            #Final Layer\n",
    "            \n",
    "\n",
    "            opt =hyperP[\"opt\"](hyperP[\"lr\"])\n",
    "            modelH.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            print (\"Model Created\")\n",
    "            modelH.summary()\n",
    "        \n",
    "            return modelH\n",
    "        \n",
    "    def get_data(self):\n",
    "        return self.train_X,self.valid_X,self.train_label,self.valid_label, self.x_test, self.y_test\n",
    "        \n",
    "    \n",
    "    \n",
    "    def create_validation_data(self):\n",
    "        self.train_X,self.valid_X,self.train_label,self.valid_label = train_test_split(self.x_train, self.y_train, \n",
    "                                                           test_size=0.2, random_state=13)\n",
    "        #print(train_X.shape,valid_X.shape,train_label.shape,valid_label.shape)\n",
    "        \n",
    "        \n",
    "    \n",
    "    # Convert to \"one-hot\" vectors using the to_categorical function\n",
    "    def one_hot_encoding(self):\n",
    "        \n",
    "        self.x_train = self.x_train/255\n",
    "        self.x_test = self.x_test/255\n",
    "        self.x_train = self.x_train.reshape( -1, 28,28,1)\n",
    "        self.x_test = self.x_test.reshape( -1, 28,28,1)\n",
    "        self.y_train = to_categorical(self.y_train)\n",
    "        self.y_test = to_categorical(self.y_test)\n",
    "        print(\"Shape after normalizing and one hot encoding\")\n",
    "        print(self.x_train.shape, self.x_test.shape, self.y_train.shape, self.y_test.shape)\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "   \n",
    "        \n",
    "    def plot_accuracy_graph(self, history):\n",
    "        evaluation = pd.DataFrame(history.history)\n",
    "        evaluation[['accuracy', 'val_accuracy']].plot()\n",
    "        evaluation[['loss', 'val_loss']].plot()\n",
    "\n",
    "                \n",
    "    def predict(self, img, model):\n",
    "        result = np.around(model.predict(img)[0])\n",
    "        max1 = max(result)\n",
    "        return np.where(result == max1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training label shape:  (60000,)\n",
      "Training Data Shape (60000, 28, 28)\n",
      "Shape after normalizing and one hot encoding\n",
      "(60000, 28, 28, 1) (10000, 28, 28, 1) (60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "fd = FashionData()\n",
    "fd.one_hot_encoding()\n",
    "fd.create_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               200832    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 202,570\n",
      "Trainable params: 202,506\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelH= fd.MakeGenericModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,valid_X,train_label,valid_label, x_test, y_test=fd.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.71564, saving model to Weights-001--1.71564.hdf5\n",
      "19/19 - 6s - loss: 0.8573 - accuracy: 0.6986 - val_loss: 1.7156 - val_accuracy: 0.6494\n",
      "Epoch 2/200\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.71564 to 1.66785, saving model to Weights-002--1.66785.hdf5\n",
      "19/19 - 7s - loss: 0.4425 - accuracy: 0.8424 - val_loss: 1.6678 - val_accuracy: 0.7151\n",
      "Epoch 3/200\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.66785 to 1.61514, saving model to Weights-003--1.61514.hdf5\n",
      "19/19 - 6s - loss: 0.3773 - accuracy: 0.8657 - val_loss: 1.6151 - val_accuracy: 0.7915\n",
      "Epoch 4/200\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.61514 to 1.55723, saving model to Weights-004--1.55723.hdf5\n",
      "19/19 - 6s - loss: 0.3414 - accuracy: 0.8764 - val_loss: 1.5572 - val_accuracy: 0.8037\n",
      "Epoch 5/200\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.55723 to 1.48315, saving model to Weights-005--1.48315.hdf5\n",
      "19/19 - 6s - loss: 0.3123 - accuracy: 0.8882 - val_loss: 1.4832 - val_accuracy: 0.7933\n",
      "Epoch 6/200\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.48315 to 1.42671, saving model to Weights-006--1.42671.hdf5\n",
      "19/19 - 6s - loss: 0.2985 - accuracy: 0.8909 - val_loss: 1.4267 - val_accuracy: 0.8048\n",
      "Epoch 7/200\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.42671 to 1.35058, saving model to Weights-007--1.35058.hdf5\n",
      "19/19 - 6s - loss: 0.2859 - accuracy: 0.8957 - val_loss: 1.3506 - val_accuracy: 0.7946\n",
      "Epoch 8/200\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.35058 to 1.28502, saving model to Weights-008--1.28502.hdf5\n",
      "19/19 - 6s - loss: 0.2670 - accuracy: 0.9038 - val_loss: 1.2850 - val_accuracy: 0.8151\n",
      "Epoch 9/200\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.28502 to 1.20181, saving model to Weights-009--1.20181.hdf5\n",
      "19/19 - 6s - loss: 0.2580 - accuracy: 0.9058 - val_loss: 1.2018 - val_accuracy: 0.8195\n",
      "Epoch 10/200\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.20181 to 1.12880, saving model to Weights-010--1.12880.hdf5\n",
      "19/19 - 6s - loss: 0.2429 - accuracy: 0.9135 - val_loss: 1.1288 - val_accuracy: 0.8168\n",
      "Epoch 11/200\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.12880 to 1.05593, saving model to Weights-011--1.05593.hdf5\n",
      "19/19 - 6s - loss: 0.2351 - accuracy: 0.9157 - val_loss: 1.0559 - val_accuracy: 0.8331\n",
      "Epoch 12/200\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.05593 to 0.98736, saving model to Weights-012--0.98736.hdf5\n",
      "19/19 - 5s - loss: 0.2278 - accuracy: 0.9186 - val_loss: 0.9874 - val_accuracy: 0.8171\n",
      "Epoch 13/200\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.98736 to 0.90912, saving model to Weights-013--0.90912.hdf5\n",
      "19/19 - 5s - loss: 0.2196 - accuracy: 0.9227 - val_loss: 0.9091 - val_accuracy: 0.8314\n",
      "Epoch 14/200\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.90912 to 0.85420, saving model to Weights-014--0.85420.hdf5\n",
      "19/19 - 5s - loss: 0.2090 - accuracy: 0.9261 - val_loss: 0.8542 - val_accuracy: 0.8267\n",
      "Epoch 15/200\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.85420 to 0.77677, saving model to Weights-015--0.77677.hdf5\n",
      "19/19 - 5s - loss: 0.1982 - accuracy: 0.9297 - val_loss: 0.7768 - val_accuracy: 0.8296\n",
      "Epoch 16/200\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.77677 to 0.73018, saving model to Weights-016--0.73018.hdf5\n",
      "19/19 - 6s - loss: 0.1937 - accuracy: 0.9322 - val_loss: 0.7302 - val_accuracy: 0.8389\n",
      "Epoch 17/200\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.73018 to 0.67734, saving model to Weights-017--0.67734.hdf5\n",
      "19/19 - 5s - loss: 0.1905 - accuracy: 0.9328 - val_loss: 0.6773 - val_accuracy: 0.8347\n",
      "Epoch 18/200\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.67734 to 0.60856, saving model to Weights-018--0.60856.hdf5\n",
      "19/19 - 5s - loss: 0.1825 - accuracy: 0.9359 - val_loss: 0.6086 - val_accuracy: 0.8423\n",
      "Epoch 19/200\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.60856 to 0.57506, saving model to Weights-019--0.57506.hdf5\n",
      "19/19 - 5s - loss: 0.1762 - accuracy: 0.9395 - val_loss: 0.5751 - val_accuracy: 0.8428\n",
      "Epoch 20/200\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.57506 to 0.53639, saving model to Weights-020--0.53639.hdf5\n",
      "19/19 - 5s - loss: 0.1675 - accuracy: 0.9420 - val_loss: 0.5364 - val_accuracy: 0.8467\n",
      "Epoch 21/200\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.53639 to 0.50584, saving model to Weights-021--0.50584.hdf5\n",
      "19/19 - 5s - loss: 0.1647 - accuracy: 0.9420 - val_loss: 0.5058 - val_accuracy: 0.8512\n",
      "Epoch 22/200\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.50584 to 0.45908, saving model to Weights-022--0.45908.hdf5\n",
      "19/19 - 5s - loss: 0.1575 - accuracy: 0.9463 - val_loss: 0.4591 - val_accuracy: 0.8582\n",
      "Epoch 23/200\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.45908 to 0.43419, saving model to Weights-023--0.43419.hdf5\n",
      "19/19 - 5s - loss: 0.1536 - accuracy: 0.9473 - val_loss: 0.4342 - val_accuracy: 0.8580\n",
      "Epoch 24/200\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.43419 to 0.40628, saving model to Weights-024--0.40628.hdf5\n",
      "19/19 - 5s - loss: 0.1490 - accuracy: 0.9497 - val_loss: 0.4063 - val_accuracy: 0.8671\n",
      "Epoch 25/200\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.40628 to 0.37266, saving model to Weights-025--0.37266.hdf5\n",
      "19/19 - 5s - loss: 0.1415 - accuracy: 0.9523 - val_loss: 0.3727 - val_accuracy: 0.8758\n",
      "Epoch 26/200\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.37266 to 0.33057, saving model to Weights-026--0.33057.hdf5\n",
      "19/19 - 5s - loss: 0.1353 - accuracy: 0.9548 - val_loss: 0.3306 - val_accuracy: 0.8916\n",
      "Epoch 27/200\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.33057 to 0.32512, saving model to Weights-027--0.32512.hdf5\n",
      "19/19 - 5s - loss: 0.1302 - accuracy: 0.9570 - val_loss: 0.3251 - val_accuracy: 0.8880\n",
      "Epoch 28/200\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.32512\n",
      "19/19 - 5s - loss: 0.1284 - accuracy: 0.9571 - val_loss: 0.3302 - val_accuracy: 0.8832\n",
      "Epoch 29/200\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.32512 to 0.31640, saving model to Weights-029--0.31640.hdf5\n",
      "19/19 - 5s - loss: 0.1225 - accuracy: 0.9588 - val_loss: 0.3164 - val_accuracy: 0.8871\n",
      "Epoch 30/200\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.31640 to 0.29396, saving model to Weights-030--0.29396.hdf5\n",
      "19/19 - 5s - loss: 0.1244 - accuracy: 0.9578 - val_loss: 0.2940 - val_accuracy: 0.8966\n",
      "Epoch 31/200\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.29396 to 0.27652, saving model to Weights-031--0.27652.hdf5\n",
      "19/19 - 6s - loss: 0.1109 - accuracy: 0.9644 - val_loss: 0.2765 - val_accuracy: 0.9025\n",
      "Epoch 32/200\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.27652\n",
      "19/19 - 6s - loss: 0.1084 - accuracy: 0.9655 - val_loss: 0.2785 - val_accuracy: 0.9013\n",
      "Epoch 33/200\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.27652\n",
      "19/19 - 5s - loss: 0.0998 - accuracy: 0.9692 - val_loss: 0.2835 - val_accuracy: 0.9013\n",
      "Epoch 34/200\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.27652 to 0.26913, saving model to Weights-034--0.26913.hdf5\n",
      "19/19 - 5s - loss: 0.0970 - accuracy: 0.9707 - val_loss: 0.2691 - val_accuracy: 0.9058\n",
      "Epoch 35/200\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0909 - accuracy: 0.9730 - val_loss: 0.2721 - val_accuracy: 0.9068\n",
      "Epoch 36/200\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0886 - accuracy: 0.9745 - val_loss: 0.2735 - val_accuracy: 0.9073\n",
      "Epoch 37/200\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0855 - accuracy: 0.9746 - val_loss: 0.2804 - val_accuracy: 0.9061\n",
      "Epoch 38/200\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0823 - accuracy: 0.9759 - val_loss: 0.2800 - val_accuracy: 0.9076\n",
      "Epoch 39/200\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0767 - accuracy: 0.9791 - val_loss: 0.2889 - val_accuracy: 0.9071\n",
      "Epoch 40/200\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0737 - accuracy: 0.9800 - val_loss: 0.2868 - val_accuracy: 0.9089\n",
      "Epoch 41/200\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0747 - accuracy: 0.9783 - val_loss: 0.3063 - val_accuracy: 0.9038\n",
      "Epoch 42/200\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0686 - accuracy: 0.9814 - val_loss: 0.2993 - val_accuracy: 0.9070\n",
      "Epoch 43/200\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0652 - accuracy: 0.9833 - val_loss: 0.3107 - val_accuracy: 0.9047\n",
      "Epoch 44/200\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0631 - accuracy: 0.9823 - val_loss: 0.3185 - val_accuracy: 0.9036\n",
      "Epoch 45/200\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0603 - accuracy: 0.9846 - val_loss: 0.3283 - val_accuracy: 0.9013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0592 - accuracy: 0.9839 - val_loss: 0.3184 - val_accuracy: 0.9076\n",
      "Epoch 47/200\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0573 - accuracy: 0.9848 - val_loss: 0.3240 - val_accuracy: 0.9067\n",
      "Epoch 48/200\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0511 - accuracy: 0.9875 - val_loss: 0.3387 - val_accuracy: 0.9048\n",
      "Epoch 49/200\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0547 - accuracy: 0.9852 - val_loss: 0.3503 - val_accuracy: 0.8981\n",
      "Epoch 50/200\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0553 - accuracy: 0.9834 - val_loss: 0.3340 - val_accuracy: 0.9084\n",
      "Epoch 51/200\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0464 - accuracy: 0.9883 - val_loss: 0.3358 - val_accuracy: 0.9069\n",
      "Epoch 52/200\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0415 - accuracy: 0.9909 - val_loss: 0.3396 - val_accuracy: 0.9072\n",
      "Epoch 53/200\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0434 - accuracy: 0.9899 - val_loss: 0.3565 - val_accuracy: 0.9043\n",
      "Epoch 54/200\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0434 - accuracy: 0.9890 - val_loss: 0.3522 - val_accuracy: 0.9035\n",
      "Epoch 55/200\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0369 - accuracy: 0.9923 - val_loss: 0.3587 - val_accuracy: 0.9059\n",
      "Epoch 56/200\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0344 - accuracy: 0.9932 - val_loss: 0.3671 - val_accuracy: 0.9024\n",
      "Epoch 57/200\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0356 - accuracy: 0.9919 - val_loss: 0.3650 - val_accuracy: 0.9040\n",
      "Epoch 58/200\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0318 - accuracy: 0.9940 - val_loss: 0.3624 - val_accuracy: 0.9058\n",
      "Epoch 59/200\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0316 - accuracy: 0.9939 - val_loss: 0.3680 - val_accuracy: 0.9050\n",
      "Epoch 60/200\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0275 - accuracy: 0.9954 - val_loss: 0.3812 - val_accuracy: 0.9013\n",
      "Epoch 61/200\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0255 - accuracy: 0.9966 - val_loss: 0.3717 - val_accuracy: 0.9077\n",
      "Epoch 62/200\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0256 - accuracy: 0.9960 - val_loss: 0.3748 - val_accuracy: 0.9071\n",
      "Epoch 63/200\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0236 - accuracy: 0.9970 - val_loss: 0.3813 - val_accuracy: 0.9060\n",
      "Epoch 64/200\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0215 - accuracy: 0.9977 - val_loss: 0.3936 - val_accuracy: 0.9035\n",
      "Epoch 65/200\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0245 - accuracy: 0.9957 - val_loss: 0.3879 - val_accuracy: 0.9045\n",
      "Epoch 66/200\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0212 - accuracy: 0.9976 - val_loss: 0.3916 - val_accuracy: 0.9050\n",
      "Epoch 67/200\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0182 - accuracy: 0.9987 - val_loss: 0.3901 - val_accuracy: 0.9077\n",
      "Epoch 68/200\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0171 - accuracy: 0.9987 - val_loss: 0.3991 - val_accuracy: 0.9043\n",
      "Epoch 69/200\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0173 - accuracy: 0.9985 - val_loss: 0.4048 - val_accuracy: 0.9073\n",
      "Epoch 70/200\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0166 - accuracy: 0.9987 - val_loss: 0.4023 - val_accuracy: 0.9059\n",
      "Epoch 71/200\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0151 - accuracy: 0.9990 - val_loss: 0.4066 - val_accuracy: 0.9055\n",
      "Epoch 72/200\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0134 - accuracy: 0.9994 - val_loss: 0.4183 - val_accuracy: 0.9040\n",
      "Epoch 73/200\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0141 - accuracy: 0.9991 - val_loss: 0.4132 - val_accuracy: 0.9061\n",
      "Epoch 74/200\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0124 - accuracy: 0.9994 - val_loss: 0.4176 - val_accuracy: 0.9059\n",
      "Epoch 75/200\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0118 - accuracy: 0.9995 - val_loss: 0.4203 - val_accuracy: 0.9061\n",
      "Epoch 76/200\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0109 - accuracy: 0.9998 - val_loss: 0.4219 - val_accuracy: 0.9052\n",
      "Epoch 77/200\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0107 - accuracy: 0.9996 - val_loss: 0.4286 - val_accuracy: 0.9038\n",
      "Epoch 78/200\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0102 - accuracy: 0.9997 - val_loss: 0.4309 - val_accuracy: 0.9051\n",
      "Epoch 79/200\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0103 - accuracy: 0.9996 - val_loss: 0.4316 - val_accuracy: 0.9065\n",
      "Epoch 80/200\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0098 - accuracy: 0.9998 - val_loss: 0.4324 - val_accuracy: 0.9051\n",
      "Epoch 81/200\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0089 - accuracy: 0.9998 - val_loss: 0.4378 - val_accuracy: 0.9052\n",
      "Epoch 82/200\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0083 - accuracy: 0.9999 - val_loss: 0.4408 - val_accuracy: 0.9068\n",
      "Epoch 83/200\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0081 - accuracy: 0.9998 - val_loss: 0.4450 - val_accuracy: 0.9062\n",
      "Epoch 84/200\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0077 - accuracy: 0.9999 - val_loss: 0.4461 - val_accuracy: 0.9064\n",
      "Epoch 85/200\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0077 - accuracy: 0.9999 - val_loss: 0.4482 - val_accuracy: 0.9053\n",
      "Epoch 86/200\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0075 - accuracy: 0.9998 - val_loss: 0.4547 - val_accuracy: 0.9039\n",
      "Epoch 87/200\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0073 - accuracy: 0.9999 - val_loss: 0.4567 - val_accuracy: 0.9057\n",
      "Epoch 88/200\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.4616 - val_accuracy: 0.9031\n",
      "Epoch 89/200\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0066 - accuracy: 0.9999 - val_loss: 0.4604 - val_accuracy: 0.9052\n",
      "Epoch 90/200\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.4693 - val_accuracy: 0.9044\n",
      "Epoch 91/200\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0062 - accuracy: 0.9999 - val_loss: 0.4651 - val_accuracy: 0.9055\n",
      "Epoch 92/200\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0054 - accuracy: 0.9999 - val_loss: 0.4687 - val_accuracy: 0.9044\n",
      "Epoch 93/200\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.4714 - val_accuracy: 0.9045\n",
      "Epoch 94/200\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.4736 - val_accuracy: 0.9053\n",
      "Epoch 95/200\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.4770 - val_accuracy: 0.9048\n",
      "Epoch 96/200\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0047 - accuracy: 0.9999 - val_loss: 0.4773 - val_accuracy: 0.9052\n",
      "Epoch 97/200\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.4797 - val_accuracy: 0.9053\n",
      "Epoch 98/200\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.4850 - val_accuracy: 0.9035\n",
      "Epoch 99/200\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.4858 - val_accuracy: 0.9057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.4901 - val_accuracy: 0.9042\n",
      "Epoch 101/200\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4892 - val_accuracy: 0.9050\n",
      "Epoch 102/200\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4928 - val_accuracy: 0.9049\n",
      "Epoch 103/200\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4999 - val_accuracy: 0.9034\n",
      "Epoch 104/200\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4985 - val_accuracy: 0.9049\n",
      "Epoch 105/200\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.4999 - val_accuracy: 0.9054\n",
      "Epoch 106/200\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5061 - val_accuracy: 0.9024\n",
      "Epoch 107/200\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.5048 - val_accuracy: 0.9051\n",
      "Epoch 108/200\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5055 - val_accuracy: 0.9049\n",
      "Epoch 109/200\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.5079 - val_accuracy: 0.9054\n",
      "Epoch 110/200\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.5104 - val_accuracy: 0.9057\n",
      "Epoch 111/200\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5127 - val_accuracy: 0.9059\n",
      "Epoch 112/200\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5163 - val_accuracy: 0.9055\n",
      "Epoch 113/200\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5197 - val_accuracy: 0.9050\n",
      "Epoch 114/200\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.5176 - val_accuracy: 0.9051\n",
      "Epoch 115/200\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5203 - val_accuracy: 0.9071\n",
      "Epoch 116/200\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5213 - val_accuracy: 0.9050\n",
      "Epoch 117/200\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.5253 - val_accuracy: 0.9045\n",
      "Epoch 118/200\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5267 - val_accuracy: 0.9054\n",
      "Epoch 119/200\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5272 - val_accuracy: 0.9056\n",
      "Epoch 120/200\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.5312 - val_accuracy: 0.9048\n",
      "Epoch 121/200\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.5311 - val_accuracy: 0.9053\n",
      "Epoch 122/200\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.5391 - val_accuracy: 0.9029\n",
      "Epoch 123/200\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5355 - val_accuracy: 0.9044\n",
      "Epoch 124/200\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5366 - val_accuracy: 0.9056\n",
      "Epoch 125/200\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5391 - val_accuracy: 0.9057\n",
      "Epoch 126/200\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5396 - val_accuracy: 0.9060\n",
      "Epoch 127/200\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5429 - val_accuracy: 0.9058\n",
      "Epoch 128/200\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5441 - val_accuracy: 0.9061\n",
      "Epoch 129/200\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5511 - val_accuracy: 0.9034\n",
      "Epoch 130/200\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5511 - val_accuracy: 0.9051\n",
      "Epoch 131/200\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5500 - val_accuracy: 0.9049\n",
      "Epoch 132/200\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5509 - val_accuracy: 0.9052\n",
      "Epoch 133/200\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5539 - val_accuracy: 0.9058\n",
      "Epoch 134/200\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.26913\n",
      "19/19 - 6s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5567 - val_accuracy: 0.9055\n",
      "Epoch 135/200\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5554 - val_accuracy: 0.9056\n",
      "Epoch 136/200\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5599 - val_accuracy: 0.9049\n",
      "Epoch 137/200\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5606 - val_accuracy: 0.9058\n",
      "Epoch 138/200\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5617 - val_accuracy: 0.9051\n",
      "Epoch 139/200\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5632 - val_accuracy: 0.9054\n",
      "Epoch 140/200\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5646 - val_accuracy: 0.9055\n",
      "Epoch 141/200\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5660 - val_accuracy: 0.9054\n",
      "Epoch 142/200\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5684 - val_accuracy: 0.9053\n",
      "Epoch 143/200\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.9054\n",
      "Epoch 144/200\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.26913\n",
      "19/19 - 5s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5712 - val_accuracy: 0.9047\n",
      "Epoch 145/200\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.26913\n",
      "19/19 - 6s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5736 - val_accuracy: 0.9042\n",
      "Epoch 146/200\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5738 - val_accuracy: 0.9056\n",
      "Epoch 147/200\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5750 - val_accuracy: 0.9051\n",
      "Epoch 148/200\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.5773 - val_accuracy: 0.9057\n",
      "Epoch 149/200\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.5778 - val_accuracy: 0.9055\n",
      "Epoch 150/200\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 9.9144e-04 - accuracy: 1.0000 - val_loss: 0.5811 - val_accuracy: 0.9055\n",
      "Epoch 151/200\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 9.5786e-04 - accuracy: 1.0000 - val_loss: 0.5819 - val_accuracy: 0.9050\n",
      "Epoch 152/200\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 9.4270e-04 - accuracy: 1.0000 - val_loss: 0.5831 - val_accuracy: 0.9051\n",
      "Epoch 153/200\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 9.5850e-04 - accuracy: 1.0000 - val_loss: 0.5849 - val_accuracy: 0.9040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 9.3075e-04 - accuracy: 1.0000 - val_loss: 0.5861 - val_accuracy: 0.9044\n",
      "Epoch 155/200\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 8.8603e-04 - accuracy: 1.0000 - val_loss: 0.5886 - val_accuracy: 0.9043\n",
      "Epoch 156/200\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 8.9009e-04 - accuracy: 1.0000 - val_loss: 0.5898 - val_accuracy: 0.9048\n",
      "Epoch 157/200\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 8.4627e-04 - accuracy: 1.0000 - val_loss: 0.5904 - val_accuracy: 0.9050\n",
      "Epoch 158/200\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 8.3640e-04 - accuracy: 1.0000 - val_loss: 0.5919 - val_accuracy: 0.9045\n",
      "Epoch 159/200\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 8.1282e-04 - accuracy: 1.0000 - val_loss: 0.5934 - val_accuracy: 0.9053\n",
      "Epoch 160/200\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 8.0777e-04 - accuracy: 1.0000 - val_loss: 0.5949 - val_accuracy: 0.9055\n",
      "Epoch 161/200\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 7.9820e-04 - accuracy: 1.0000 - val_loss: 0.5971 - val_accuracy: 0.9051\n",
      "Epoch 162/200\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 7.7183e-04 - accuracy: 1.0000 - val_loss: 0.5982 - val_accuracy: 0.9053\n",
      "Epoch 163/200\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 7.6282e-04 - accuracy: 1.0000 - val_loss: 0.6011 - val_accuracy: 0.9049\n",
      "Epoch 164/200\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 7.4525e-04 - accuracy: 1.0000 - val_loss: 0.6017 - val_accuracy: 0.9048\n",
      "Epoch 165/200\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 7.5612e-04 - accuracy: 1.0000 - val_loss: 0.6030 - val_accuracy: 0.9039\n",
      "Epoch 166/200\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 7.3139e-04 - accuracy: 1.0000 - val_loss: 0.6043 - val_accuracy: 0.9055\n",
      "Epoch 167/200\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 7.0311e-04 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 0.9049\n",
      "Epoch 168/200\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 6.8425e-04 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 0.9050\n",
      "Epoch 169/200\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 6.7734e-04 - accuracy: 1.0000 - val_loss: 0.6071 - val_accuracy: 0.9044\n",
      "Epoch 170/200\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 6.5392e-04 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 0.9045\n",
      "Epoch 171/200\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 6.3943e-04 - accuracy: 1.0000 - val_loss: 0.6105 - val_accuracy: 0.9050\n",
      "Epoch 172/200\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 6.3704e-04 - accuracy: 1.0000 - val_loss: 0.6115 - val_accuracy: 0.9051\n",
      "Epoch 173/200\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 6.2116e-04 - accuracy: 1.0000 - val_loss: 0.6132 - val_accuracy: 0.9039\n",
      "Epoch 174/200\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 6.0191e-04 - accuracy: 1.0000 - val_loss: 0.6144 - val_accuracy: 0.9044\n",
      "Epoch 175/200\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.9003e-04 - accuracy: 1.0000 - val_loss: 0.6161 - val_accuracy: 0.9048\n",
      "Epoch 176/200\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.9902e-04 - accuracy: 1.0000 - val_loss: 0.6182 - val_accuracy: 0.9047\n",
      "Epoch 177/200\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.9544e-04 - accuracy: 1.0000 - val_loss: 0.6204 - val_accuracy: 0.9041\n",
      "Epoch 178/200\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.9778e-04 - accuracy: 1.0000 - val_loss: 0.6205 - val_accuracy: 0.9047\n",
      "Epoch 179/200\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.5922e-04 - accuracy: 1.0000 - val_loss: 0.6215 - val_accuracy: 0.9046\n",
      "Epoch 180/200\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.3737e-04 - accuracy: 1.0000 - val_loss: 0.6218 - val_accuracy: 0.9039\n",
      "Epoch 181/200\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.2941e-04 - accuracy: 1.0000 - val_loss: 0.6243 - val_accuracy: 0.9045\n",
      "Epoch 182/200\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.2125e-04 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 0.9040\n",
      "Epoch 183/200\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.1290e-04 - accuracy: 1.0000 - val_loss: 0.6256 - val_accuracy: 0.9042\n",
      "Epoch 184/200\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.0174e-04 - accuracy: 1.0000 - val_loss: 0.6289 - val_accuracy: 0.9048\n",
      "Epoch 185/200\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 5.0073e-04 - accuracy: 1.0000 - val_loss: 0.6290 - val_accuracy: 0.9042\n",
      "Epoch 186/200\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 4.9357e-04 - accuracy: 1.0000 - val_loss: 0.6295 - val_accuracy: 0.9038\n",
      "Epoch 187/200\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 4.8383e-04 - accuracy: 1.0000 - val_loss: 0.6310 - val_accuracy: 0.9046\n",
      "Epoch 188/200\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 4.7365e-04 - accuracy: 1.0000 - val_loss: 0.6337 - val_accuracy: 0.9039\n",
      "Epoch 189/200\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 4.6027e-04 - accuracy: 1.0000 - val_loss: 0.6343 - val_accuracy: 0.9042\n",
      "Epoch 190/200\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.26913\n",
      "19/19 - 8s - loss: 4.5486e-04 - accuracy: 1.0000 - val_loss: 0.6347 - val_accuracy: 0.9041\n",
      "Epoch 191/200\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 4.4886e-04 - accuracy: 1.0000 - val_loss: 0.6354 - val_accuracy: 0.9034\n",
      "Epoch 192/200\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.26913\n",
      "19/19 - 8s - loss: 4.3749e-04 - accuracy: 1.0000 - val_loss: 0.6382 - val_accuracy: 0.9039\n",
      "Epoch 193/200\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.26913\n",
      "19/19 - 8s - loss: 4.3017e-04 - accuracy: 1.0000 - val_loss: 0.6389 - val_accuracy: 0.9043\n",
      "Epoch 194/200\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 4.2660e-04 - accuracy: 1.0000 - val_loss: 0.6411 - val_accuracy: 0.9045\n",
      "Epoch 195/200\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 4.1861e-04 - accuracy: 1.0000 - val_loss: 0.6413 - val_accuracy: 0.9041\n",
      "Epoch 196/200\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 4.1021e-04 - accuracy: 1.0000 - val_loss: 0.6430 - val_accuracy: 0.9038\n",
      "Epoch 197/200\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 4.0750e-04 - accuracy: 1.0000 - val_loss: 0.6439 - val_accuracy: 0.9042\n",
      "Epoch 198/200\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 3.9374e-04 - accuracy: 1.0000 - val_loss: 0.6452 - val_accuracy: 0.9042\n",
      "Epoch 199/200\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 3.8739e-04 - accuracy: 1.0000 - val_loss: 0.6469 - val_accuracy: 0.9046\n",
      "Epoch 200/200\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.26913\n",
      "19/19 - 7s - loss: 3.8482e-04 - accuracy: 1.0000 - val_loss: 0.6469 - val_accuracy: 0.9039\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 3, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "history = modelH.fit(train_X, train_label, batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, validation_split=.2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy  :  0.7198958396911621\n",
      "Testing Accuracy  :  0.7128999829292297\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy\", \" : \", modelH.evaluate(train_X, train_label, verbose=False)[1])\n",
    "print(\"Testing Accuracy\",  \" : \", modelH.evaluate(x_test, y_test, verbose=False)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction :  T-shirt/top\n"
     ]
    }
   ],
   "source": [
    "index = 10\n",
    "img = x_test[index]\n",
    "img.shape = (28,28)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "print(\"Prediction : \", classes[fd.predict(x_test[index:index+1], modelH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fd.plot_accuracy_graph(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_29 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_23 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_31 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lr =[0.1, 0.01,  1, 0.5, 0.05]\n",
    "newModel1=[]\n",
    "for i in range(len(lr)):\n",
    "    params = {\n",
    "        #Training Level Hyper Params\n",
    "        \"epochs\": 2,\n",
    "        \"lr\": i,\n",
    "        \"batch_size\": 2048,\n",
    "        \"opt\": keras.optimizers.Adam,\n",
    "        #Network Level HyperParam\n",
    "        \"conv_layers\":{\n",
    "            1: [Conv2D(32, kernel_size=(3, 3), activation='linear', input_shape=(28,28,1),padding='same')],\n",
    "            2: [LeakyReLU(alpha=0.1)],\n",
    "            3: [MaxPooling2D((2, 2),padding='same')],\n",
    "        },\n",
    "        \"conn_layers\":{\n",
    "            1: [Dense(128, activation='linear')],\n",
    "            2: [LeakyReLU(alpha=0.1)],\n",
    "            3: [Dense(10, activation='softmax')],\n",
    "        },\n",
    "    }\n",
    "    newModel1.append(fd.MakeGenericModel(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.75758\n",
      "19/19 - 6s - loss: 2.3002 - accuracy: 0.0833 - val_loss: 2.3024 - val_accuracy: 0.0817\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.75758\n",
      "19/19 - 7s - loss: 2.3002 - accuracy: 0.0833 - val_loss: 2.3024 - val_accuracy: 0.0817\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.75758\n",
      "19/19 - 7s - loss: 82976.3438 - accuracy: 0.1581 - val_loss: 96351.1484 - val_accuracy: 0.1904\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.75758\n",
      "19/19 - 7s - loss: 54149.2734 - accuracy: 0.4443 - val_loss: 16806.6914 - val_accuracy: 0.6054\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.75758\n",
      "19/19 - 7s - loss: 684783.3750 - accuracy: 0.1661 - val_loss: 580135.6250 - val_accuracy: 0.1856\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.75758\n",
      "19/19 - 7s - loss: 358744.0000 - accuracy: 0.3128 - val_loss: 105514.7500 - val_accuracy: 0.4646\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.75758\n",
      "19/19 - 7s - loss: 2066343.8750 - accuracy: 0.1702 - val_loss: 1069886.8750 - val_accuracy: 0.4412\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.75758\n",
      "19/19 - 7s - loss: 557721.5000 - accuracy: 0.5204 - val_loss: 166286.4062 - val_accuracy: 0.6335\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 1.75758\n",
      "19/19 - 7s - loss: 6299113.5000 - accuracy: 0.1389 - val_loss: 3737214.7500 - val_accuracy: 0.2967\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.75758\n",
      "19/19 - 7s - loss: 2609563.0000 - accuracy: 0.4274 - val_loss: 821796.1875 - val_accuracy: 0.6453\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lr)):\n",
    "    newModel1[i].fit(train_X, train_label, batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, validation_split=.2,callbacks=callbacks_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 0.1  :  0.08297916501760483\n",
      "Testing Accuracy 0.1  :  0.08810000121593475\n",
      "Training Accuracy 0.01  :  0.6074791550636292\n",
      "Testing Accuracy 0.01  :  0.600600004196167\n",
      "Training Accuracy 1  :  0.4672708213329315\n",
      "Testing Accuracy 1  :  0.4657999873161316\n",
      "Training Accuracy 0.5  :  0.6334999799728394\n",
      "Testing Accuracy 0.5  :  0.6251000165939331\n",
      "Training Accuracy 0.05  :  0.6498333215713501\n",
      "Testing Accuracy 0.05  :  0.6371999979019165\n",
      "Best learning rate 0.05  with accuracy of  0.6371999979019165\n"
     ]
    }
   ],
   "source": [
    "test_acc=[]\n",
    "for i in range(len(lr)):\n",
    "    print(\"Training Accuracy\", lr[i], \" : \", newModel1[i].evaluate(train_X, train_label, verbose=False)[1])\n",
    "    print(\"Testing Accuracy\", lr[i], \" : \", newModel1[i].evaluate(x_test, y_test, verbose=False)[1])\n",
    "    test_acc.append(newModel1[i].evaluate(x_test, y_test, verbose=False)[1])\n",
    "max_index = np.argmax(test_acc)\n",
    "key = lr[max_index]\n",
    "print(\"Best learning rate\", key, \" with accuracy of \", test_acc[max_index])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For No. of Conv and Maxpool Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_35 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_35 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 128)               200832    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 211,690\n",
      "Trainable params: 211,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_35 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 14, 14, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)   (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 7, 7, 32)          9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)   (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 85,770\n",
      "Trainable params: 85,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "no_layers = [3,6,9]\n",
    "layers = {\n",
    "    1: [Conv2D(32, kernel_size=(3, 3), activation = \"linear\", input_shape=(28,28,1),padding='same')],\n",
    "    2: [LeakyReLU(alpha=0.1)],\n",
    "    3: [MaxPooling2D((2, 2),padding='same')],\n",
    "    4: [Conv2D(32, kernel_size=(3, 3), activation = \"linear\", input_shape=(28,28,1),padding='same')],\n",
    "    5: [LeakyReLU(alpha=0.1)],\n",
    "    6: [MaxPooling2D((2, 2),padding='same')],\n",
    "    7: [Conv2D(32, kernel_size=(3, 3), activation = \"linear\", input_shape=(28,28,1),padding='same')],\n",
    "    8: [LeakyReLU(alpha=0.1)],\n",
    "    9: [MaxPooling2D((2, 2),padding='same')],\n",
    "}\n",
    "newModel2=[]\n",
    "for i in no_layers:\n",
    "    params = {\n",
    "        #Training Level Hyper Params\n",
    "        \"epochs\": 2,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_size\": 2048,\n",
    "        \"opt\": keras.optimizers.Adam,\n",
    "        #Network Level HyperParam\n",
    "        \"conv_layers\":dict((i, layers[i]) for i in range(1,i+1)),\n",
    "        \"conn_layers\":{\n",
    "            1: [Dense(128, activation='linear')],\n",
    "            2: [LeakyReLU(alpha=0.1)],\n",
    "            3: [Dense(10, activation='softmax')],\n",
    "        },\n",
    "    }\n",
    "    newModel2.append(fd.MakeGenericModel(params))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss improved from 1.75758 to 0.59618, saving model to Weights-001--0.59618.hdf5\n",
      "19/19 - 6s - loss: 1.0356 - accuracy: 0.6655 - val_loss: 0.5962 - val_accuracy: 0.7878\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59618 to 0.46414, saving model to Weights-002--0.46414.hdf5\n",
      "19/19 - 7s - loss: 0.5220 - accuracy: 0.8177 - val_loss: 0.4641 - val_accuracy: 0.8446\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.46414\n",
      "19/19 - 11s - loss: 1.3606 - accuracy: 0.5612 - val_loss: 0.7273 - val_accuracy: 0.7382\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.46414\n",
      "19/19 - 11s - loss: 0.6426 - accuracy: 0.7688 - val_loss: 0.5571 - val_accuracy: 0.7948\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.46414\n",
      "19/19 - 12s - loss: 1.3739 - accuracy: 0.5538 - val_loss: 0.7884 - val_accuracy: 0.7083\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.46414\n",
      "19/19 - 12s - loss: 0.7106 - accuracy: 0.7398 - val_loss: 0.6299 - val_accuracy: 0.7665\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(no_layers)):\n",
    "    newModel2[i].fit(train_X, train_label, batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, validation_split=.2,callbacks=callbacks_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 3  :  0.7942916750907898\n",
      "Testing Accuracy 3  :  0.7822999954223633\n",
      "Training Accuracy 6  :  0.7757916450500488\n",
      "Testing Accuracy 6  :  0.7702000141143799\n",
      "Training Accuracy 9  :  0.7706458568572998\n",
      "Testing Accuracy 9  :  0.7646999955177307\n",
      "Best no of layers are  3  with accuracy of  0.7822999954223633\n"
     ]
    }
   ],
   "source": [
    "test_acc=[]\n",
    "for i in range(0, len(no_layers)):\n",
    "    print(\"Training Accuracy\", no_layers[i], \" : \", newModel2[i].evaluate(train_X, train_label, verbose=False)[1])\n",
    "    print(\"Testing Accuracy\", no_layers[i], \" : \", newModel2[i].evaluate(x_test, y_test, verbose=False)[1])\n",
    "    test_acc.append(newModel2[i].evaluate(x_test, y_test, verbose=False)[1])\n",
    "max_index = np.argmax(test_acc)\n",
    "key = no_layers[max_index]\n",
    "print(\"Best no of layers are \", key, \" with accuracy of \", test_acc[max_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For no. of Fully Connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [<tensorflow.python.keras.layers.core.Dense object at 0x0000019A0C666C70>], 2: [<tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000019A0C5D6EB0>], 3: [<tensorflow.python.keras.layers.core.Dense object at 0x0000019A0C6A00D0>]}\n",
      "Model Created\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_38 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "{1: [<tensorflow.python.keras.layers.core.Dense object at 0x0000019A0C666C70>], 2: [<tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000019A0C5D6EB0>], 3: [<tensorflow.python.keras.layers.core.Dense object at 0x0000019A0C5D6640>], 4: [<tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000019A0C6A0A90>], 5: [<tensorflow.python.keras.layers.core.Dense object at 0x0000019A0C680700>]}\n",
      "Model Created\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_39 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 821,066\n",
      "Trainable params: 821,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "{1: [<tensorflow.python.keras.layers.core.Dense object at 0x0000019A0C666C70>], 2: [<tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000019A0C5D6EB0>], 3: [<tensorflow.python.keras.layers.core.Dense object at 0x0000019A0C5D6640>], 4: [<tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000019A0C6A0A90>], 5: [<tensorflow.python.keras.layers.core.Dense object at 0x0000019A0C6A0A30>], 6: [<tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x0000019A0C6A0040>], 7: [<tensorflow.python.keras.layers.core.Dense object at 0x0000019A0C6A3670>]}\n",
      "Model Created\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_40 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 837,578\n",
      "Trainable params: 837,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "no_layers_conn = [2,4,6]\n",
    "layers1 = {\n",
    "    1: [Dense(128, activation='linear')],\n",
    "    2: [LeakyReLU(alpha=0.1)],\n",
    "    3: [Dense(128, activation='linear')],\n",
    "    4: [LeakyReLU(alpha=0.1)],\n",
    "    5: [Dense(128, activation='linear')],\n",
    "    6: [LeakyReLU(alpha=0.1)],\n",
    "}\n",
    "newModel3=[]\n",
    "\n",
    "for i in no_layers_conn:\n",
    "    temp = i+1\n",
    "    dict1 = dict((i, layers1[i]) for i in range(1,i+1))\n",
    "    dict1.update({temp: [Dense(10, activation='softmax')]})\n",
    "    print(dict1)\n",
    "    params = {\n",
    "        #Training Level Hyper Params\n",
    "        \"epochs\": 2,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_size\": 2048,\n",
    "        \"opt\": keras.optimizers.Adam,\n",
    "        #Network Level HyperParam\n",
    "        \"conv_layers\":{\n",
    "            1: [Conv2D(32, kernel_size=(3, 3), activation='linear', input_shape=(28,28,1),padding='same')],\n",
    "            2: [LeakyReLU(alpha=0.1)],\n",
    "            3: [MaxPooling2D((2, 2),padding='same')],\n",
    "        },\n",
    "        \"conn_layers\": dict1,\n",
    "    }\n",
    "    newModel3.append(fd.MakeGenericModel(params))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.46414\n",
      "19/19 - 6s - loss: 1.0425 - accuracy: 0.6684 - val_loss: 0.5802 - val_accuracy: 0.7993\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.46414 to 0.44697, saving model to Weights-002--0.44697.hdf5\n",
      "19/19 - 6s - loss: 0.5079 - accuracy: 0.8240 - val_loss: 0.4470 - val_accuracy: 0.8425\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.44697\n",
      "19/19 - 7s - loss: 1.4140 - accuracy: 0.5503 - val_loss: 0.7113 - val_accuracy: 0.7584\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44697\n",
      "19/19 - 7s - loss: 0.5887 - accuracy: 0.7951 - val_loss: 0.5138 - val_accuracy: 0.8110\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.44697\n",
      "19/19 - 7s - loss: 1.3136 - accuracy: 0.5786 - val_loss: 0.6626 - val_accuracy: 0.7628\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44697\n",
      "19/19 - 7s - loss: 0.5741 - accuracy: 0.7900 - val_loss: 0.5156 - val_accuracy: 0.8068\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(no_layers_conn)):\n",
    "    newModel3[i].fit(train_X, train_label, batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, validation_split=.2,callbacks=callbacks_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy 2  :  0.6620625257492065\n",
      "Testing Accuracy 2  :  0.6521999835968018\n",
      "Training Accuracy 4  :  0.6551666855812073\n",
      "Testing Accuracy 4  :  0.6514000296592712\n",
      "Training Accuracy 6  :  0.8040624856948853\n",
      "Testing Accuracy 6  :  0.7965999841690063\n",
      "Best no of fully connected layers are  6  with accuracy of  0.7965999841690063\n"
     ]
    }
   ],
   "source": [
    "test_acc=[]\n",
    "for i in range(0, len(no_layers)):\n",
    "    print(\"Training Accuracy\", no_layers_conn[i], \" : \", newModel3[i].evaluate(train_X, train_label, verbose=False)[1])\n",
    "    print(\"Testing Accuracy\", no_layers_conn[i], \" : \", newModel3[i].evaluate(x_test, y_test, verbose=False)[1])\n",
    "    test_acc.append(newModel3[i].evaluate(x_test, y_test, verbose=False)[1])\n",
    "max_index = np.argmax(test_acc)\n",
    "key = no_layers_conn[max_index]\n",
    "print(\"Best no of fully connected layers are \", key, \" with accuracy of \", test_acc[max_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_41 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_35 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_42 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_81 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_36 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_82 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_43 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_83 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_84 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_44 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_85 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_44 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_86 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_45 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_87 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_45 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_88 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "newModel4=[]\n",
    "for i in range(len(hyperparams.optimizers)):\n",
    "    params = {\n",
    "        #Training Level Hyper Params\n",
    "        \"epochs\": 2,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_size\": 2048,\n",
    "        \"opt\": hyperparams.optimizers[i],\n",
    "        #Network Level HyperParam\n",
    "        \"conv_layers\":{\n",
    "            1: [Conv2D(32, kernel_size=(3, 3), activation='linear', input_shape=(28,28,1),padding='same')],\n",
    "            2: [LeakyReLU(alpha=0.1)],\n",
    "            3: [MaxPooling2D((2, 2),padding='same')],\n",
    "        },\n",
    "        \"conn_layers\":{\n",
    "            1: [Dense(128, activation='linear')],\n",
    "            2: [LeakyReLU(alpha=0.1)],\n",
    "            3: [Dense(10, activation='softmax')],\n",
    "        },\n",
    "    }\n",
    "    newModel4.append(fd.MakeGenericModel(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.44697\n",
      "19/19 - 7s - loss: 1.0682 - accuracy: 0.6667 - val_loss: 0.5836 - val_accuracy: 0.7977\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44697 to 0.44342, saving model to Weights-002--0.44342.hdf5\n",
      "19/19 - 7s - loss: 0.5023 - accuracy: 0.8266 - val_loss: 0.4434 - val_accuracy: 0.8453\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.44342\n",
      "19/19 - 7s - loss: 1.1103 - accuracy: 0.6318 - val_loss: 0.6381 - val_accuracy: 0.7786\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44342\n",
      "19/19 - 8s - loss: 0.6218 - accuracy: 0.7752 - val_loss: 0.6801 - val_accuracy: 0.7879\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.44342\n",
      "19/19 - 8s - loss: 2.2996 - accuracy: 0.1551 - val_loss: 2.2816 - val_accuracy: 0.1934\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44342\n",
      "19/19 - 7s - loss: 2.2664 - accuracy: 0.2433 - val_loss: 2.2501 - val_accuracy: 0.2868\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.44342\n",
      "19/19 - 7s - loss: 1.1556 - accuracy: 0.6687 - val_loss: 0.6373 - val_accuracy: 0.7800\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44342\n",
      "19/19 - 8s - loss: 0.5586 - accuracy: 0.8084 - val_loss: 0.5049 - val_accuracy: 0.8227\n",
      "Epoch 1/2\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.44342\n",
      "19/19 - 8s - loss: 1.1926 - accuracy: 0.6348 - val_loss: 0.6836 - val_accuracy: 0.7607\n",
      "Epoch 2/2\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.44342\n",
      "19/19 - 8s - loss: 0.5878 - accuracy: 0.7883 - val_loss: 0.5242 - val_accuracy: 0.8071\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(hyperparams.optimizers)):\n",
    "    newModel4[i].fit(train_X, train_label, batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, validation_split=.2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>  :  0.8455208539962769\n",
      "Testing Accuracy <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>  :  0.8343999981880188\n",
      "Training Accuracy <class 'tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop'>  :  0.7883333563804626\n",
      "Testing Accuracy <class 'tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop'>  :  0.7828999757766724\n",
      "Training Accuracy <class 'tensorflow.python.keras.optimizer_v2.adagrad.Adagrad'>  :  0.29006248712539673\n",
      "Testing Accuracy <class 'tensorflow.python.keras.optimizer_v2.adagrad.Adagrad'>  :  0.28439998626708984\n",
      "Training Accuracy <class 'tensorflow.python.keras.optimizer_v2.adamax.Adamax'>  :  0.8248124718666077\n",
      "Testing Accuracy <class 'tensorflow.python.keras.optimizer_v2.adamax.Adamax'>  :  0.8127999901771545\n",
      "Training Accuracy <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>  :  0.8142499923706055\n",
      "Testing Accuracy <class 'tensorflow.python.keras.optimizer_v2.nadam.Nadam'>  :  0.803600013256073\n",
      "Best Optimizer is  <class 'tensorflow.python.keras.optimizer_v2.adam.Adam'>  with accuracy of  0.8343999981880188\n"
     ]
    }
   ],
   "source": [
    "test_acc=[]\n",
    "for i in range(len(hyperparams.optimizers)):\n",
    "    print(\"Training Accuracy\",hyperparams.optimizers[i] , \" : \", newModel4[i].evaluate(train_X, train_label, verbose=False)[1])\n",
    "    print(\"Testing Accuracy\", hyperparams.optimizers[i], \" : \", newModel4[i].evaluate(x_test, y_test, verbose=False)[1])\n",
    "    test_acc.append(newModel4[i].evaluate(x_test, y_test, verbose=False)[1])\n",
    "max_index = np.argmax(test_acc)\n",
    "key = hyperparams.optimizers[max_index]\n",
    "print(\"Best Optimizer is \", key, \" with accuracy of \", test_acc[max_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Created\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-8fc2ba7144ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         },\n\u001b[0;32m     16\u001b[0m         \"conn_layers\":{\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "newModel5=[]\n",
    "for i in range(len(hyperparams.activation)):\n",
    "    params = {\n",
    "        #Training Level Hyper Params\n",
    "        \"epochs\": 2,\n",
    "        \"lr\": 0.001,\n",
    "        \"batch_size\": 2048,\n",
    "        \"opt\": keras.optimizers.Adam,\n",
    "        \n",
    "        #Network Level HyperParam\n",
    "        \"conv_layers\":{\n",
    "            1: [Conv2D(32, kernel_size=(3, 3), activation=hyperparams.activation[i], input_shape=(28,28,1),padding='same') ],\n",
    "            2: [LeakyReLU(alpha=0.1)],\n",
    "            \n",
    "            3: [MaxPooling2D((2, 2),padding='same')],\n",
    "        },\n",
    "        \"conn_layers\":{\n",
    "            1: [Dense(128, activation='linear', kernel)],\n",
    "            2: [LeakyReLU(alpha=0.1)],\n",
    "            3: [Dense(10, activation='softmax')],\n",
    "        },\n",
    "    }\n",
    "    newModel5.append(fd.MakeGenericModel(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Created\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_20 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    #Training Level Hyper Params\n",
    "    \"epochs\": 20,\n",
    "    \"lr\": 1,\n",
    "    \"batch_size\": 2048,\n",
    "    \"opt\": keras.optimizers.Adam,\n",
    "     #l22 = tf.keras.regularizers.l2(l2=0.02)\n",
    "    #Network Level HyperParam\n",
    "    \"conv_layers\":{\n",
    "        1: [Conv2D(32, kernel_size=(3, 3), activation='linear', input_shape=(28,28,1),padding='same', kernel_regularizer=\"l2\") ],\n",
    "        2: [LeakyReLU(alpha=0.1)],\n",
    "        3: [MaxPooling2D((2, 2),padding='same')],\n",
    "        4: [keras.layers.Dropout(0.2)]\n",
    "    },\n",
    "    \"conn_layers\":{\n",
    "        1: [Dense(128, activation='linear', kernel_regularizer=\"l2\")],\n",
    "        2: [LeakyReLU(alpha=0.1)],\n",
    "        3: [keras.layers.Dropout(0.2)],\n",
    "        4: [Dense(10, activation='softmax')],\n",
    "     },\n",
    "    }\n",
    "modelH = fd.MakeGenericModel(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.62623\n",
      "19/19 - 9s - loss: 130411.8438 - accuracy: 0.1768 - val_loss: 79366.2500 - val_accuracy: 0.5329\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.62623\n",
      "19/19 - 10s - loss: 68006.9062 - accuracy: 0.5080 - val_loss: 41164.7188 - val_accuracy: 0.6899\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.62623\n",
      "19/19 - 9s - loss: 29075.3672 - accuracy: 0.6712 - val_loss: 14782.7686 - val_accuracy: 0.7375\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 13876.7900 - accuracy: 0.6313 - val_loss: 10307.5117 - val_accuracy: 0.6758\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 9601.0742 - accuracy: 0.6470 - val_loss: 6054.5195 - val_accuracy: 0.7654\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 5756.6450 - accuracy: 0.6753 - val_loss: 9959.6035 - val_accuracy: 0.4596\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 15602.0312 - accuracy: 0.5562 - val_loss: 12604.7627 - val_accuracy: 0.6943\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 9580.1152 - accuracy: 0.6780 - val_loss: 4382.0962 - val_accuracy: 0.7846\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 3510.6125 - accuracy: 0.6899 - val_loss: 1948.8320 - val_accuracy: 0.7648\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 2267.6626 - accuracy: 0.6818 - val_loss: 7171.7402 - val_accuracy: 0.3342\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 8474.1299 - accuracy: 0.5742 - val_loss: 6644.2051 - val_accuracy: 0.7066\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 4791.0962 - accuracy: 0.6810 - val_loss: 1914.5298 - val_accuracy: 0.7928\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 5035.0083 - accuracy: 0.5944 - val_loss: 28488.3867 - val_accuracy: 0.3855\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 22283.9434 - accuracy: 0.5465 - val_loss: 16155.7900 - val_accuracy: 0.6354\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 9952.8223 - accuracy: 0.6717 - val_loss: 4605.8877 - val_accuracy: 0.7084\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 6066.8276 - accuracy: 0.6062 - val_loss: 5955.6084 - val_accuracy: 0.7341\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 3746.0850 - accuracy: 0.7029 - val_loss: 1507.7365 - val_accuracy: 0.7279\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 1271.4666 - accuracy: 0.6539 - val_loss: 1632.1439 - val_accuracy: 0.5790\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 2223.4634 - accuracy: 0.6522 - val_loss: 1342.5944 - val_accuracy: 0.7867\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.62623\n",
      "19/19 - 8s - loss: 891.6337 - accuracy: 0.7078 - val_loss: 1537.4325 - val_accuracy: 0.5085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19a1061a3d0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelH.fit(train_X, train_label, batch_size=params['batch_size'], epochs=params['epochs'], verbose=2, validation_split=.2,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
